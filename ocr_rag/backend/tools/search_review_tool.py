"""
æœç´¢ç»“æœå®¡æŸ¥å·¥å…·
ç”¨äºåˆ¤å®šä»ç½‘ç»œæœç´¢å¾—åˆ°çš„æ–°é—»/ç»“æœæ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ï¼Œè§£å†³ä»…åŸºäºå…³é”®è¯çš„è¯¯åˆ¤ä»¥åŠæ—¶é—´ä¸ä¸€è‡´é—®é¢˜ã€‚

ä¸»è¦ç­–ç•¥ï¼ˆè½»é‡ã€æ— éœ€å¤–éƒ¨ä¾èµ–ï¼‰ï¼š
- è§£æ `web_search` æ ¼å¼åŒ–æ–‡æœ¬ä¸ºç»“æ„åŒ–æ¡ç›®
- å¯¹æ¯æ¡ç»“æœè®¡ç®—ï¼šå…³é”®è¯é‡åˆå¾—åˆ† + æ—¶é—´ä¸€è‡´æ€§å¾—åˆ†
- è¾“å‡ºç»“æ„åŒ– JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«æ¯æ¡ç»“æœçš„è¯„åˆ†ã€åŸå› ï¼Œä»¥åŠæ¨èä½¿ç”¨çš„ç»“æœç´¢å¼•åˆ—è¡¨

æ­¤å·¥å…·ä»¥ `@tool` å¯¼å‡ºï¼Œä¾›æ¨¡å‹åœ¨å·¥å…·é“¾ä¸­è°ƒç”¨ã€‚
"""
import re
import json
from datetime import datetime
from typing import List, Dict, Any
from loguru import logger
from langchain_core.tools import tool


def _tokenize(text: str) -> List[str]:
    text = text or ""
    # ç®€å•æ‹†åˆ†å¹¶å»æ‰å¸¸è§æ ‡ç‚¹
    tokens = re.findall(r"[\w\u4e00-\u9fff]+", text.lower())
    # å»æ‰éå¸¸çŸ­çš„è¯
    return [t for t in tokens if len(t) > 1]


def _parse_formatted_results(formatted: str) -> List[Dict[str, Any]]:
    """
    è§£æ `WebSearchTool.format_results` äº§å‡ºçš„æ–‡æœ¬æ ¼å¼ä¸ºç»“æ„åŒ–æ¡ç›®ã€‚
    æ”¯æŒçš„å­—æ®µï¼šindex, title, snippet, url, source
    """
    if not formatted:
        return []

    entries = []
    # æ¯æ¡è®°å½•ä»¥ "[i] title" å¼€å¤´ï¼Œæ¥ç€æœ‰ä¸€è¡Œä»¥ "ğŸ“" å¼€å¤´çš„ snippetï¼Œå¯èƒ½æœ‰ "ğŸ”— url"ï¼Œæœ€åæœ‰ "ğŸ“ æ¥æº: source"
    pattern = re.compile(
        r"\[(?P<index>\d+)\]\s*(?P<title>.*?)\nğŸ“\s*(?P<snippet>.*?)(?:\nğŸ”—\s*(?P<url>.*?))?\nğŸ“ æ¥æº:\s*(?P<source>.*?)\n\n",
        re.S
    )

    for m in pattern.finditer(formatted):
        entries.append({
            "index": int(m.group("index")),
            "title": (m.group("title") or "").strip(),
            "snippet": (m.group("snippet") or "").strip(),
            "url": (m.group("url") or "").strip(),
            "source": (m.group("source") or "").strip(),
        })

    # å¦‚æœæ²¡æœ‰è§£æåˆ°ï¼ˆæ ¼å¼ä¸åŒï¼‰ï¼Œå°è¯•æŒ‰ç©ºè¡Œåˆ†å—è§£ææœ€å°ä¿¡æ¯
    if not entries:
        blocks = [b.strip() for b in formatted.split('\n\n') if b.strip()]
        for i, b in enumerate(blocks, 1):
            lines = b.split('\n')
            title = lines[0] if lines else f"ç»“æœ {i}"
            snippet = ''
            url = ''
            source = ''
            for ln in lines[1:]:
                if ln.startswith('ğŸ“'):
                    snippet = ln.replace('ğŸ“', '').strip()
                elif ln.startswith('ğŸ”—'):
                    url = ln.replace('ğŸ”—', '').strip()
                elif 'æ¥æº' in ln or 'æ¥æº:' in ln:
                    source = ln.split(':')[-1].strip()

            entries.append({
                'index': i,
                'title': title,
                'snippet': snippet,
                'url': url,
                'source': source,
            })

    return entries


def _date_mentioned(text: str) -> List[str]:
    """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾å¯èƒ½çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å›å‘ç°çš„æ—¥æœŸç‰‡æ®µ"""
    if not text:
        return []
    patterns = [
        r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",
        r"\d{4}-\d{1,2}-\d{1,2}",
        r"\d{1,2}æœˆ\d{1,2}æ—¥",
    ]
    found = []
    for p in patterns:
        found += re.findall(p, text)
    return found


def _compute_relevance_score(question: str, title: str, snippet: str) -> float:
    """åŸºäºå…³é”®è¯é‡åˆè®¡ç®—ç®€å•ç›¸å…³æ€§å¾—åˆ†ï¼ˆ0-1ï¼‰"""
    q_tokens = set(_tokenize(question))
    doc_tokens = set(_tokenize(title + ' ' + snippet))
    if not q_tokens or not doc_tokens:
        return 0.0
    inter = q_tokens & doc_tokens
    union = q_tokens | doc_tokens
    return len(inter) / len(union)


def _compute_recency_score(current_date: str, title: str, snippet: str) -> float:
    """
    ç®€å•æ—¶é—´ä¸€è‡´æ€§è¯„åˆ†ï¼š
    - å¦‚æœç‰‡æ®µä¸­å‡ºç°å½“å‰æ—¥æœŸ -> 1.0
    - å¦‚æœå‡ºç°æœ€è¿‘/æ—¥å‰/å°æ—¶ ç­‰æç¤ºè¯ -> 0.8
    - å¦‚æœå‡ºç°å¹´ä»½å¹¶ä¸”ä¸å½“å‰å¹´ä»½ç›¸åŒ -> 0.6
    - å¦åˆ™ 0.3ï¼ˆæœªçŸ¥æ—¶é—´ï¼‰
    """
    now_year = None
    try:
        if current_date:
            # æ”¯æŒ 'YYYY-MM-DD' æˆ– 'YYYYå¹´MMæœˆDDæ—¥' çš„è§£æ
            if 'å¹´' in current_date:
                now_year = int(re.search(r"(\d{4})å¹´", current_date).group(1))
            else:
                now_year = int(current_date.split('-')[0])
    except Exception:
        now_year = None

    text = (title or '') + ' ' + (snippet or '')
    # ç›´æ¥åŒ…å«å®Œæ•´å½“å‰æ—¥æœŸ
    if current_date and (current_date in text or current_date.replace('-', 'å¹´') in text):
        return 1.0

    # å«æœ‰â€œæœ€è¿‘/æ—¥å‰/å°æ—¶/ä»Šå¤©/æ˜¨æ—¥/æ˜¨å¤©/æœ¬å‘¨/æœ¬æœˆâ€ç­‰è¯
    if re.search(r"æœ€è¿‘|æ—¥å‰|å°æ—¶|ä»Šå¤©|æ˜¨æ—¥|æ˜¨å¤©|æœ¬å‘¨|æœ¬æœˆ|åˆšåˆš", text):
        return 0.8

    # æŸ¥æ‰¾å¹´ä»½
    years = re.findall(r"(\d{4})å¹´", text)
    if years:
        try:
            if now_year and int(years[0]) == now_year:
                return 0.6
            else:
                return 0.2
        except Exception:
            return 0.2

    return 0.3


@tool
async def review_search_results(formatted_results: str, user_question: str, current_date: str = '') -> str:
    """
    å®¡æŸ¥æœç´¢ç»“æœï¼šåˆ¤æ–­å“ªäº›ç»“æœä¸ç”¨æˆ·é—®é¢˜ç›¸å…³å¹¶ç»™å‡ºç†ç”±ã€‚

    Args:
        formatted_results: æ¥è‡ª `web_search` çš„æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆæˆ–å…¶ä»–ç±»ä¼¼æ–‡æœ¬ï¼‰
        user_question: ç”¨æˆ·åŸå§‹é—®é¢˜/ä¸Šä¸‹æ–‡
        current_date: å¯é€‰ï¼Œä¼ å…¥å½“å‰æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ '2025-11-22' æˆ– '2025å¹´11æœˆ22æ—¥'ï¼‰ï¼Œç”¨äºæ—¶é—´ä¸€è‡´æ€§åˆ¤æ–­

    Returns:
        JSON å­—ç¬¦ä¸²ï¼Œç»“æ„å¦‚ä¸‹ï¼š
        {
          "summary": "ç®€çŸ­å®¡æŸ¥ç»“è®º",
          "recommendations": [1,3],
          "entries": [ {index, title, snippet, url, source, relevance_score, recency_score, final_score, reasons: []}, ... ]
        }
    """
    try:
        entries = _parse_formatted_results(formatted_results)

        results = []
        rec_list = []

        for e in entries:
            title = e.get('title', '')
            snippet = e.get('snippet', '')
            idx = e.get('index')

            rel = _compute_relevance_score(user_question, title, snippet)
            rec = _compute_recency_score(current_date or '', title, snippet)

            # æœ€ç»ˆåˆ†æ•°ï¼šå…³é”®è¯ç›¸å…³æ€§å æ¯” 0.7ï¼Œæ—¶é—´ä¸€è‡´æ€§ 0.3
            final = rel * 0.7 + rec * 0.3

            reasons = []
            if rel > 0.4:
                reasons.append(f"å…³é”®è¯åŒ¹é…({rel:.2f})")
            else:
                reasons.append(f"å…³é”®è¯åŒ¹é…å¼±({rel:.2f})")

            if rec >= 0.8:
                reasons.append("æ—¶é—´ä¿¡æ¯ä¸æŸ¥è¯¢é«˜åº¦ä¸€è‡´")
            elif rec >= 0.5:
                reasons.append("æ—¶é—´å¯èƒ½ç›¸å…³")
            else:
                reasons.append("æ—¶é—´ä¸æ˜ç¡®æˆ–è¾ƒæ—§")

            results.append({
                'index': idx,
                'title': title,
                'snippet': snippet,
                'url': e.get('url', ''),
                'source': e.get('source', ''),
                'relevance_score': round(rel, 3),
                'recency_score': round(rec, 3),
                'final_score': round(final, 3),
                'reasons': reasons,
            })

        # æ¨èï¼šé€‰å– final_score >= threshold æˆ– top-N
        threshold = 0.4
        recommended = [r['index'] for r in results if r['final_score'] >= threshold]

        # å¦‚æœæ²¡æœ‰ä»»ä½•è¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™å– top2
        if not recommended and results:
            sorted_by_score = sorted(results, key=lambda x: x['final_score'], reverse=True)
            recommended = [sorted_by_score[i]['index'] for i in range(min(2, len(sorted_by_score)))]

        summary = f"å…±è§£æåˆ° {len(results)} æ¡ç»“æœï¼Œæ¨èä½¿ç”¨ {len(recommended)} æ¡ã€‚"

        output = {
            'summary': summary,
            'recommendations': recommended,
            'entries': results,
            'metadata': {
                'checked_at': datetime.now().isoformat(),
                'question_tokens': len(_tokenize(user_question))
            }
        }

        logger.info(f"ğŸ” å®¡æŸ¥å®Œæˆï¼š{summary}")
        return json.dumps(output, ensure_ascii=False)

    except Exception as e:
        logger.error(f"å®¡æŸ¥å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
        return json.dumps({'error': str(e)}, ensure_ascii=False)


# å¯¼å‡ºå·¥å…·åˆ—è¡¨ä»¥ä¾¿è¢« main.py å¯¼å…¥ç»‘å®š
REVIEW_TOOLS = [
    review_search_results
]
