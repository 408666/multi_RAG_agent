"""
ç½‘é¡µæŠ“å–å·¥å…·æ¨¡å—
ç”¨äºè®¿é—®ç‰¹å®šURLå¹¶æå–ç½‘é¡µå†…å®¹
"""
import asyncio
import requests
from bs4 import BeautifulSoup
from typing import Optional, Dict, Any
from urllib.parse import urlparse, urljoin
from loguru import logger
from langchain_core.tools import tool


class WebScraper:
    """ç½‘é¡µæŠ“å–å·¥å…·ç±»"""

    def __init__(self, timeout: int = 30, max_content_length: int = 50000):
        """
        åˆå§‹åŒ–ç½‘é¡µæŠ“å–å·¥å…·

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦é™åˆ¶
        """
        self.timeout = timeout
        self.max_content_length = max_content_length
        self.session = requests.Session()

        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

    def _is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except Exception:
            return False

    def _extract_main_content(self, soup: BeautifulSoup, url: str) -> str:
        """
        ä»HTMLä¸­æå–ä¸»è¦å†…å®¹

        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: åŸå§‹URL

        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
            element.decompose()

        # å°è¯•å¤šç§ç­–ç•¥æå–ä¸»è¦å†…å®¹
        content_selectors = [
            'main',
            '[role="main"]',
            '.main-content',
            '.content',
            '.article-content',
            '.post-content',
            '#main',
            '#content',
            '.entry-content'
        ]

        main_content = None
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break

        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
        if not main_content:
            main_content = soup.find('body') or soup

        # æå–æ–‡æœ¬
        text = main_content.get_text(separator='\n', strip=True)

        # æ¸…ç†æ–‡æœ¬
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        cleaned_text = '\n\n'.join(lines)

        return cleaned_text

    def _get_page_info(self, soup: BeautifulSoup, url: str) -> Dict[str, str]:
        """è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯"""
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"

        # å°è¯•è·å–æè¿°
        description = ""
        meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
        if meta_desc:
            description = meta_desc.get('content', '').strip()

        return {
            'title': title_text,
            'description': description,
            'url': url
        }

    async def scrape_url(self, url: str) -> Dict[str, Any]:
        """
        æŠ“å–æŒ‡å®šURLçš„å†…å®¹

        Args:
            url: è¦æŠ“å–çš„URL

        Returns:
            åŒ…å«é¡µé¢ä¿¡æ¯å’Œå†…å®¹çš„å­—å…¸
        """
        if not self._is_valid_url(url):
            return {
                'error': f'æ— æ•ˆçš„URL: {url}',
                'content': '',
                'title': '',
                'description': '',
                'url': url
            }

        try:
            logger.info(f"ğŸŒ å¼€å§‹æŠ“å–ç½‘é¡µ: {url}")

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è¯·æ±‚
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.session.get(url, timeout=self.timeout, allow_redirects=True)
            )

            response.raise_for_status()

            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' not in content_type:
                return {
                    'error': f'ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {content_type}',
                    'content': '',
                    'title': '',
                    'description': '',
                    'url': url
                }

            # è§£æHTML
            soup = BeautifulSoup(response.content, 'lxml')

            # è·å–é¡µé¢ä¿¡æ¯
            page_info = self._get_page_info(soup, url)

            # æå–ä¸»è¦å†…å®¹
            content = self._extract_main_content(soup, url)

            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(content) > self.max_content_length:
                content = content[:self.max_content_length] + "...\n\n[å†…å®¹å·²æˆªæ–­]"

            result = {
                'title': page_info['title'],
                'description': page_info['description'],
                'content': content,
                'url': url,
                'status_code': response.status_code,
                'content_length': len(content)
            }

            logger.info(f"âœ… ç½‘é¡µæŠ“å–æˆåŠŸ: {page_info['title']} ({len(content)} å­—ç¬¦)")
            return result

        except requests.exceptions.RequestException as e:
            error_msg = f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return {
                'error': error_msg,
                'content': '',
                'title': '',
                'description': '',
                'url': url
            }
        except Exception as e:
            error_msg = f"ç½‘é¡µæŠ“å–å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return {
                'error': error_msg,
                'content': '',
                'title': '',
                'description': '',
                'url': url
            }


# åˆ›å»ºå…¨å±€æŠ“å–å·¥å…·å®ä¾‹
_scraper_instance = None


def get_web_scraper() -> WebScraper:
    """è·å–ç½‘é¡µæŠ“å–å·¥å…·å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _scraper_instance
    if _scraper_instance is None:
        _scraper_instance = WebScraper()
    return _scraper_instance


@tool
async def fetch_webpage(url: str) -> str:
    """
    è®¿é—®æŒ‡å®šçš„ç½‘é¡µURLå¹¶æå–å…¶å†…å®¹

    Args:
        url: è¦è®¿é—®çš„å®Œæ•´URLï¼Œå¿…é¡»åŒ…å«http://æˆ–https://

    Returns:
        æ ¼å¼åŒ–çš„ç½‘é¡µå†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æè¿°å’Œæ­£æ–‡

    Examples:
        - "https://www.example.com/article"
        - "https://news.sina.com.cn/c/2024-01-01/doc-abc123"
    """
    logger.info(f"ğŸ”— è®¿é—®ç½‘é¡µ: {url}")

    try:
        scraper = get_web_scraper()
        result = await scraper.scrape_url(url)

        if 'error' in result:
            return f"âŒ è®¿é—®å¤±è´¥: {result['error']}"

        # æ ¼å¼åŒ–è¾“å‡º
        formatted = f"ğŸ“„ ç½‘é¡µæ ‡é¢˜: {result['title']}\n"
        if result['description']:
            formatted += f"ğŸ“ é¡µé¢æè¿°: {result['description']}\n"
        formatted += f"ğŸ”— URL: {result['url']}\n"
        formatted += f"ğŸ“Š å†…å®¹é•¿åº¦: {result['content_length']} å­—ç¬¦\n\n"
        formatted += f"ğŸ“– é¡µé¢å†…å®¹:\n{'-'*50}\n{result['content']}"

        return formatted

    except Exception as e:
        error_msg = f"ç½‘é¡µæŠ“å–å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        return error_msg


# å¯¼å‡ºå·¥å…·åˆ—è¡¨
WEB_SCRAPING_TOOLS = [
    fetch_webpage
]