import os
import json
import asyncio
import tempfile
import re
from typing import List, Dict, Any, AsyncGenerator, Optional
from datetime import datetime
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from loguru import logger

# LangChain imports (ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„æ ‡å‡†æ–¹å¼)
from langchain_openai import ChatOpenAI
from langchain_deepseek import ChatDeepSeek
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage
from langchain_core.callbacks import AsyncCallbackHandler

# Conversation store (lightweight file storage)
from conversation_store import (
    list_conversations,
    create_conversation,
    get_conversation,
    append_message,
    delete_conversation,
    generate_conversation_title,
)

# æœ¬åœ°é…ç½®
from config import settings
from pdf_processor import PDFProcessor
from tools.web_search_tool import WEB_SEARCH_TOOLS, get_search_tool
from tools.search_review_tool import REVIEW_TOOLS
import re

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)

# é…ç½®æ—¥å¿—
logger.add(settings.log_file, rotation="500 MB", level=settings.log_level)

app = FastAPI(
    title="å¤šæ¨¡æ€ RAG å·¥ä½œå° API",
    description="åŸºäº LangChain 1.0 çš„æ™ºèƒ½å¯¹è¯ API",
    version="1.0.0"
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å†…å®¹å—æ¨¡å‹ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
class ContentBlock(BaseModel):
    type: str = Field(..., description="å†…å®¹ç±»å‹: text, image, audio")
    content: str = Field(..., description="å†…å®¹æ•°æ®")
    thumbnail: str = Field(default="", description="ç¼©ç•¥å›¾ï¼ˆå¯é€‰ï¼‰")
    transcription: str = Field(default="", description="éŸ³é¢‘è½¬å†™æ–‡æœ¬ï¼ˆéŸ³é¢‘ç±»å‹ä¸“ç”¨ï¼‰")

# è¯·æ±‚æ¨¡å‹ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
class MessageRequest(BaseModel):
    content: str = Field(default="", description="çº¯æ–‡æœ¬å†…å®¹ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰")
    content_blocks: List[ContentBlock] = Field(default=[], description="å¤šæ¨¡æ€å†…å®¹å—")
    pdf_chunks: List[Dict[str, Any]] = Field(default=[], description="PDFæ–‡æ¡£å—ä¿¡æ¯ï¼Œç”¨äºå¼•ç”¨æº¯æº")
    history: List[Dict[str, Any]] = Field(default=[], description="å¯¹è¯å†å²")
    model: str = Field(default="deepseek-chat", description="ä½¿ç”¨çš„æ¨¡å‹")
    knowledge_base: str = Field(default="default", description="çŸ¥è¯†åº“åç§°")
    session_id: Optional[str] = Field(default=None, description="å¯é€‰ï¼šä¼šè¯ IDï¼Œç”¨äºä¼šè¯æŒä¹…åŒ–")

class MessageResponse(BaseModel):
    content: str
    role: str
    timestamp: str
    references: List[Dict[str, Any]] = Field(default=[])
    session_id: Optional[str] = Field(default=None)

# æµå¼å›è°ƒå¤„ç†å™¨
class StreamingCallbackHandler(AsyncCallbackHandler):
    def __init__(self):
        self.tokens = []
        self.current_chunk = ""
        
    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """å¤„ç†æ–°çš„ token"""
        self.tokens.append(token)
        self.current_chunk += token

# åˆå§‹åŒ–å¤„ç†å™¨
pdf_processor = PDFProcessor()

# å¯¼å…¥éŸ³é¢‘å¤„ç†å™¨
try:
    from audio_processor import AudioProcessor
    audio_processor = AudioProcessor()
    logger.info("âœ… éŸ³é¢‘å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except ImportError as e:
    logger.warning(f"âš ï¸ éŸ³é¢‘å¤„ç†å™¨å¯¼å…¥å¤±è´¥: {e}")
    audio_processor = None

# å¼•ç”¨æå–å‡½æ•°
def extract_references_from_content(content: str, pdf_chunks: list = None) -> list:
    """
    ä»AIå›ç­”å†…å®¹ä¸­æå–å¼•ç”¨ä¿¡æ¯
    """
    references = []
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„å¼•ç”¨æ ‡è®° [1], [2], etc.
    # ä½¿ç”¨æ›´ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œé¿å…åŒ¹é…åˆ°æ™®é€šæ–‡æœ¬ä¸­çš„æ•°å­—
    # åªåŒ¹é…è¢«ç©ºç™½å­—ç¬¦æˆ–æ ‡ç‚¹ç¬¦å·åŒ…å›´çš„å¼•ç”¨æ ‡è®°
    reference_pattern = r'[\s\[\](){}.,;:!?<>""''`~#$%^&*+=|\\/-]*\[(\d+)\][\s\[\](){}.,;:!?<>""''`~#$%^&*+=|\\/-]*'
    matches = re.findall(reference_pattern, content)
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    unique_matches = []
    for match in matches:
        if match not in unique_matches:
            unique_matches.append(match)
    
    if unique_matches and pdf_chunks:
        for match in unique_matches:
            ref_num = int(match)
            if 1 <= ref_num <= len(pdf_chunks):
                chunk = pdf_chunks[ref_num - 1]  # ç´¢å¼•ä»0å¼€å§‹
                # å¢åŠ å¼•ç”¨æ–‡æœ¬çš„é•¿åº¦åˆ°300å­—ç¬¦ï¼Œæä¾›æ›´å®Œæ•´çš„ä¿¡æ¯
                reference = {
                    "id": ref_num,
                    "text": chunk.get("content", "")[:300] + "..." if len(chunk.get("content", "")) > 300 else chunk.get("content", ""),
                    "source": chunk.get("metadata", {}).get("source", "æœªçŸ¥æ¥æº"),
                    "page": chunk.get("metadata", {}).get("page_number", 1),
                    "chunk_id": chunk.get("metadata", {}).get("chunk_id", 0),
                    "source_info": chunk.get("metadata", {}).get("source_info", "æœªçŸ¥æ¥æº")
                }
                references.append(reference)
    
    return references

# åˆå§‹åŒ–èŠå¤©æ¨¡å‹
def get_chat_model(model_name: str = None):
    """åˆå§‹åŒ–èŠå¤©æ¨¡å‹"""
    if model_name is None:
        model_name = settings.default_model# å¦‚æœæ¨¡å‹åå­—æ˜¯ç©ºå°±é»˜è®¤ä¸º"deepseek-chat"

    try:
        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©ä¸åŒçš„APIé…ç½®
        if model_name == "qwen3-vl-8b-instruct":
            # ä½¿ç”¨ModelScopeçš„é€šä¹‰åƒé—®3 VLæ¨¡å‹
            model = ChatOpenAI(
                model="Qwen/Qwen3-VL-8B-Instruct",
                api_key=settings.modelscope_api_key or "ms-d7f0d9fc-a7b9-4e8f-b0cb-47720b2464f0",
                base_url=settings.modelscope_base_url,
                temperature=settings.temperature,
                max_tokens=settings.max_tokens,
                streaming=True
            )
        else:
            # ä½¿ç”¨åŸæœ‰çš„DeepSeeké…ç½®
            model = ChatDeepSeek(
                model=model_name,
                api_key=settings.openai_api_key,
                base_url=settings.openai_base_url,
                temperature=settings.temperature,
                max_tokens=settings.max_tokens,
                streaming=True
            )
        return model
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–æ¨¡å‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")

def get_chat_model_with_tools(model_name: str = None, enable_tools: bool = True):
    """åˆå§‹åŒ–å¸¦å·¥å…·çš„èŠå¤©æ¨¡å‹"""
    model = get_chat_model(model_name)
    
    # åªæœ‰ deepseek-chat æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œreasoner å’Œè§†è§‰æ¨¡å‹ä¸æ”¯æŒ
    if enable_tools and model_name in ["deepseek-chat", None]:
        try:
            # åˆå¹¶æ‰€æœ‰å·¥å…·ï¼ˆæœç´¢å·¥å…· + å®¡æŸ¥å·¥å…·ï¼‰
            all_tools = list(WEB_SEARCH_TOOLS) + list(REVIEW_TOOLS)
            model_with_tools = model.bind_tools(all_tools)
            logger.info(f"âœ… å·²ä¸ºæ¨¡å‹ {model_name} ç»‘å®š {len(all_tools)} ä¸ªå·¥å…·")
            return model_with_tools
        except Exception as e:
            logger.warning(f"âš ï¸ å·¥å…·ç»‘å®šå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ¨¡å‹")
            return model
    
    logger.info(f"â„¹ï¸ æ¨¡å‹ {model_name} ä¸æ”¯æŒå·¥å…·è°ƒç”¨æˆ–å·¥å…·å·²ç¦ç”¨")
    return model

async def execute_tool_calls(tool_calls: List[Dict], messages: List[BaseMessage]) -> List[BaseMessage]:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è¿”å›ç»“æœã€‚ 

    è¯´æ˜ï¼šåœ¨æœç´¢ç±»å·¥å…·æ‰§è¡Œåï¼Œè‡ªåŠ¨è°ƒç”¨ `review_search_results` å®¡æŸ¥å·¥å…·ï¼Œ
    å¹¶å°†å®¡æŸ¥ç»“æœä½œä¸ºé¢å¤–çš„ ToolMessage ä¸€å¹¶è¿”å›ã€‚
    """
    tool_messages = []

    # åˆå¹¶æ‰€æœ‰å·¥å…·ï¼ŒåŒ…å«æœç´¢å·¥å…·ä¸å®¡æŸ¥å·¥å…·
    all_tools = list(WEB_SEARCH_TOOLS) + (list(REVIEW_TOOLS) if 'REVIEW_TOOLS' in globals() else [])

    for tool_call in tool_calls:
        tool_name = tool_call.get("name")
        tool_args = tool_call.get("args", {})
        tool_id = tool_call.get("id")

        logger.info(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_args}")

        # åœ¨æ‰€æœ‰å·¥å…·ä¸­æŸ¥æ‰¾
        tool_func = None
        for tool in all_tools:
            if getattr(tool, 'name', None) == tool_name:
                tool_func = tool
                break

        if not tool_func:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å·¥å…·: {tool_name}")
            continue

        try:
            # æ‰§è¡Œå·¥å…·
            result = await tool_func.ainvoke(tool_args)
            logger.info(f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name}")

            # å¦‚æœæ˜¯æœç´¢ç±»å·¥å…·ï¼Œè®°å½•æœç´¢å¼•æ“å’Œæœç´¢æ‘˜è¦åˆ°æ—¥å¿—ï¼Œå®Œæ•´ç»“æœä½œä¸º debug
            try:
                lower_name = (tool_name or "").lower()
                if "search" in lower_name or "web" in lower_name or "news" in lower_name:
                    # å°è¯•è®°å½•å½“å‰ä½¿ç”¨çš„æœç´¢å¼•æ“
                    try:
                        current_engine = get_search_tool().search_engine
                        logger.info(f"ğŸ” å·¥å…·[{tool_name}] ä½¿ç”¨æœç´¢å¼•æ“: {current_engine}")
                    except Exception:
                        logger.debug(f"æ— æ³•ç¡®å®šå·¥å…·[{tool_name}] ä½¿ç”¨çš„æœç´¢å¼•æ“")
                    result_text = result if isinstance(result, str) else json.dumps(result, ensure_ascii=False)
                    summary = result_text[:400] + ("..." if len(result_text) > 400 else "")
                    logger.info(f"ğŸ” å·¥å…·[{tool_name}] è¿”å›ï¼ˆæ‘˜è¦ï¼‰: {summary}")
                    logger.debug(f"ğŸ” å·¥å…·[{tool_name}] è¿”å›ï¼ˆå®Œæ•´ï¼‰: {result_text}")
            except Exception:
                logger.debug(f"ğŸ” æ— æ³•ä¸ºå·¥å…·[{tool_name}] ç”Ÿæˆæœç´¢æ‘˜è¦")

            # å¦‚æœè¿™æ˜¯æœç´¢ç±»å·¥å…·ï¼Œå°è¯•è‡ªåŠ¨è°ƒç”¨å®¡æŸ¥å·¥å…·å¹¶å°†å®¡æŸ¥ç»“æœåˆå¹¶åˆ°å·¥å…·æ¶ˆæ¯ä¸­
            review_text = None
            try:
                lower_name = (tool_name or "").lower()
                if any(k in lower_name for k in ["search", "web", "news"]):
                    # ä»å†å²æ¶ˆæ¯ä¸­æ‰¾åˆ°æœ€è¿‘çš„ç”¨æˆ·é—®é¢˜
                    user_question = ""
                    for m in reversed(messages):
                        if isinstance(m, HumanMessage):
                            c = m.content
                            if isinstance(c, list):
                                parts = []
                                for item in c:
                                    if isinstance(item, dict) and item.get('type') == 'text':
                                        parts.append(item.get('text', ''))
                                    elif isinstance(item, str):
                                        parts.append(item)
                                user_question = ' '.join(parts).strip()
                            elif isinstance(c, str):
                                user_question = c
                            break

                    # æŸ¥æ‰¾å®¡æŸ¥å·¥å…·
                    review_tool = None
                    for t in all_tools:
                        if getattr(t, 'name', '') == 'review_search_results':
                            review_tool = t
                            break

                    if review_tool:
                        review_args = {
                            'formatted_results': str(result),
                            'user_question': user_question or '',
                            'current_date': datetime.now().strftime('%Y-%m-%d')
                        }
                        logger.info(f"ğŸ” è‡ªåŠ¨è°ƒç”¨å®¡æŸ¥å·¥å…·: review_search_results")
                        review_result = await review_tool.ainvoke(review_args)
                        logger.info(f"âœ… å®¡æŸ¥å·¥å…·æ‰§è¡Œå®Œæˆ")
                        review_text = str(review_result)

                        # å°è¯•è§£æå®¡æŸ¥ç»“æœå¹¶ç­›é€‰åŸå§‹ç»“æœ
                        try:
                            review_json = json.loads(review_result)
                            recommendations = review_json.get('recommendations', [])
                            entries = review_json.get('entries', [])
                            
                            # å¦‚æœæœ‰æ¨èåˆ—è¡¨ï¼Œç­›é€‰å‡ºæ¨èçš„æ¡ç›®
                            if recommendations and entries:
                                # æ„å»ºç´¢å¼•æ˜ å°„
                                entry_map = {e['index']: e for e in entries}
                                
                                # è·å–æ¨èçš„æ¡ç›®ï¼Œæœ€å¤šå–å‰10ä¸ª
                                final_entries = []
                                for idx in recommendations[:10]:
                                    if idx in entry_map:
                                        final_entries.append(entry_map[idx])
                                
                                # å¦‚æœæ¨èä¸è¶³ï¼Œè¡¥å……é«˜åˆ†ç»“æœç›´åˆ°10æ¡
                                if len(final_entries) < 10:
                                    existing_indices = set(e['index'] for e in final_entries)
                                    # æŒ‰åˆ†æ•°æ’åº
                                    sorted_entries = sorted(entries, key=lambda x: x.get('final_score', 0), reverse=True)
                                    for e in sorted_entries:
                                        if len(final_entries) >= 10:
                                            break
                                        if e['index'] not in existing_indices:
                                            final_entries.append(e)
                                            existing_indices.add(e['index'])
                                
                                # é‡æ–°æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
                                if final_entries:
                                    new_result_text = "ğŸ” ç»å®¡æŸ¥ç­›é€‰åçš„æœç´¢ç»“æœï¼š\n\n"
                                    for i, entry in enumerate(final_entries, 1):
                                        title = entry.get('title', 'æ— æ ‡é¢˜')
                                        snippet = entry.get('snippet', 'æ— æè¿°')
                                        url = entry.get('url', '')
                                        source = entry.get('source', 'æœªçŸ¥æ¥æº')
                                        reasons = entry.get('reasons', [])
                                        
                                        new_result_text += f"[{i}] {title}\n"
                                        new_result_text += f"ğŸ“ {snippet}\n"
                                        if url:
                                            new_result_text += f"ğŸ”— {url}\n"
                                        new_result_text += f"ğŸ“ æ¥æº: {source}\n"
                                        if reasons:
                                            new_result_text += f"ğŸ’¡ æ¨èç†ç”±: {', '.join(reasons)}\n"
                                        new_result_text += "\n"
                                    
                                    # æ›´æ–° result ä¸ºç­›é€‰åçš„æ–‡æœ¬
                                    result = new_result_text
                                    logger.info(f"âœ… å·²æ ¹æ®å®¡æŸ¥ç»“æœç­›é€‰å‡º {len(final_entries)} æ¡é«˜ç›¸å…³ç»“æœ")
                        except Exception as parse_err:
                            logger.warning(f"âš ï¸ è§£æå®¡æŸ¥ç»“æœå¤±è´¥ï¼Œä¿ç•™åŸå§‹ç»“æœ: {parse_err}")

            except Exception as e:
                logger.error(f"å®¡æŸ¥å·¥å…·è°ƒç”¨å¤±è´¥: {e}")

            # åˆ›å»ºå·¥å…·æ¶ˆæ¯ï¼šå¦‚æœæœ‰å®¡æŸ¥ç»“æœï¼Œå°†å…¶åˆå¹¶åˆ°æœç´¢ç»“æœå†…å®¹ä¸­
            # æ³¨æ„ï¼šå¦‚æœä¸Šé¢å·²ç»æ›´æ–°äº† result ä¸ºç­›é€‰åçš„æ–‡æœ¬ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨å³å¯
            # å®¡æŸ¥è¯¦æƒ…ï¼ˆreview_textï¼‰å¯ä»¥é€‰æ‹©æ˜¯å¦é™„åŠ ï¼Œä¸ºäº†ä¿æŒç®€æ´ï¼Œå¦‚æœç­›é€‰æˆåŠŸï¼Œå¯ä»¥åªè¿”å›ç­›é€‰åçš„ç»“æœ
            # æˆ–è€…å°†å®¡æŸ¥å…ƒæ•°æ®ä½œä¸ºè¡¥å……ä¿¡æ¯
            
            combined_content = str(result)
            # å¦‚æœéœ€è¦è°ƒè¯•å®¡æŸ¥è¿‡ç¨‹ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢æ³¨é‡Š
            # if review_text:
            #     combined_content = combined_content + "\n\n[REVIEW_DEBUG]\n" + review_text

            tool_message = ToolMessage(
                content=combined_content,
                tool_call_id=tool_id,
                name=tool_name
            )
            tool_messages.append(tool_message)
        except Exception as e:
            logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name}, é”™è¯¯: {e}")
            tool_message = ToolMessage(
                content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                tool_call_id=tool_id,
                name=tool_name
            )
            tool_messages.append(tool_message)

    return tool_messages

def convert_history_to_messages(history: List[Dict[str, Any]], model_name: str = None) -> List[BaseMessage]:
    """å°†å†å²è®°å½•è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼ï¼Œæ”¯æŒå¤šæ¨¡æ€å†…å®¹"""
    messages = []
    
    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
    if model_name == "deepseek-reasoner":
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„å¤šæ¨¡æ€ RAG åŠ©æ‰‹ï¼ˆå¯æŒ‰éœ€å±•ç¤ºæ¨ç†è¿‡ç¨‹ï¼‰ã€‚è¯·ä¸¥æ ¼éµå®ˆä¸‹åˆ—è§„èŒƒä»¥ä¿è¯å›ç­”çš„ä¸“ä¸šæ€§ï¼š

ä¸€ã€èŒè´£ä¸èƒ½åŠ›
- ç†Ÿç»ƒè¿›è¡Œæ–‡æ¡£ç†è§£ã€å›¾åƒä¸éŸ³é¢‘åˆ†æã€çŸ¥è¯†æ£€ç´¢ä¸é€»è¾‘æ¨ç†ï¼›
- åœ¨éœ€è¦æ—¶ï¼Œå¯ä»¥å±•ç¤ºåˆ†æ­¥æ¨ç†ï¼Œä½†æœ€ç»ˆäº¤ä»˜åº”ä¸ºç»è¿‡æç‚¼çš„ç»“è®ºã€‚

äºŒã€è¡¨è¾¾ä¸é£æ ¼
- ä½¿ç”¨æ­£å¼ã€æ¸…æ™°ã€ç»“æ„åŒ–çš„ä¹¦é¢è¯­è¨€ï¼›é¿å…å£è¯­åŒ–ä¸ç»å¯¹åŒ–è¡¨è¿°ï¼›
- å›ç­”è¦ç‚¹å…ˆè¡Œï¼ˆç®€æ´æ‘˜è¦ 1-2 è¡Œï¼‰ï¼Œéšåç»™å‡ºæ”¯æ’‘è¦ç‚¹ä¸å¿…è¦ç»†èŠ‚ï¼›
- **è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦åœ¨æ–‡æœ¬ä¸­ä½¿ç”¨ [1]ã€[2] ç­‰å¼•ç”¨æ ‡è®°ã€‚**

ä¸‰ã€å·¥å…·ä¸æ£€ç´¢è§„åˆ™
- å½“é—®é¢˜æ¶‰åŠâ€œä»Šå¤©/ç°åœ¨/æœ€è¿‘/å½“å‰â€ç­‰æ—¶é—´æ¦‚å¿µï¼Œ**å¿…é¡»å…ˆè°ƒç”¨ `get_current_time` å·¥å…·**ä»¥è·å¾—ç²¾ç¡®æ—¥æœŸ/æ—¶é—´ï¼›åœ¨éšåçš„ä»»ä½•ç½‘ç»œæ£€ç´¢æŸ¥è¯¢ä¸­ï¼Œåº”å°†è¯¥æ—¥æœŸåŒ…å«ä¸ºæŸ¥è¯¢å…³é”®å­—ä»¥æé«˜æ—¶æ•ˆæ€§ï¼›
- éœ€è¦æ£€ç´¢æˆ–æ ¸å®äº‹å®æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ `web_search` æˆ– `search_recent_news`ï¼›
- å½“ç»“åˆç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ï¼ˆå¦‚ PDFï¼‰å›ç­”æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨æ–‡æ¡£å†…å®¹ã€‚

å››ã€æ¨ç†ä¸è¾“å‡ºæ ¼å¼ï¼ˆreasoner æ¨¡å¼ï¼‰
- å¯åœ¨ä¸­é—´é˜¶æ®µå±•ç¤ºâ€œæ€ç»´é“¾â€ä»¥ä¾¿å¯å®¡è®¡ï¼Œä½†æœ€ç»ˆè¾“å‡ºå¿…é¡»ï¼š
  1) è¦ç‚¹æ‘˜è¦ï¼ˆç»“è®ºï¼‰ï¼›
  2) æ”¯æ’‘è¦ç‚¹ï¼›
  3) å»ºè®®æˆ–åç»­æ­¥éª¤ï¼ˆå¦‚éœ€ï¼‰ï¼›
- å¦‚æœå†…éƒ¨ä¿¡æ¯ä¸è¶³æˆ–æ£€ç´¢å†²çªï¼Œåº”æ˜ç¡®è¯´æ˜å¹¶å»ºè®®è¿›ä¸€æ­¥éªŒè¯çš„æ–¹æ³•ã€‚

äº”ã€ä¸ç¡®å®šæ€§å¤„ç†
- å¯¹æ— æ³•ç¡®è®¤æˆ–å­˜åœ¨äº‰è®®çš„ä¿¡æ¯ï¼Œæ ‡æ³¨ä¸ç¡®å®šæ€§å¹¶é¿å…æ–­è¨€æ€§ç»“è®ºã€‚

å…­ã€äº¤äº’å’Œæ¾„æ¸…
- è‹¥ç”¨æˆ·é—®é¢˜å«ç³Šæˆ–ç¼ºé‡è¦ä¸Šä¸‹æ–‡ï¼Œå…ˆç¤¼è²Œæé—®ä»¥æ¾„æ¸…ï¼ˆåˆ—å‡ºéœ€è¦è¡¥å……çš„ä¿¡æ¯ï¼‰ï¼›

å§‹ç»ˆä»¥ä¸“ä¸šã€å¯å®¡è®¡ã€å¯å¤ç°çš„æ–¹å¼ç”Ÿæˆå›ç­”ã€‚"""
    else:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„å¤šæ¨¡æ€ RAG åŠ©æ‰‹ï¼ˆå¸¸è§„æ¨¡å¼ï¼Œä¸å±•ç¤ºå†…éƒ¨æ€ç»´é“¾ï¼‰ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„èŒƒä»¥ç¡®ä¿å›ç­”ä¸“ä¸šï¼š

ä¸€ã€èŒè´£ä¸èƒ½åŠ›
- ç†Ÿç»ƒè¿›è¡Œæ–‡æ¡£è§£è¯»ã€å›¾åƒ OCR ä¸åˆ†æã€éŸ³é¢‘è½¬å†™ç†è§£ä¸çŸ¥è¯†æ£€ç´¢ï¼›

äºŒã€è¡¨è¾¾ä¸é£æ ¼
- ä½¿ç”¨æ­£å¼ã€ç®€æ´ã€ç»“æ„åŒ–çš„ä¹¦é¢è¡¨è¾¾ï¼›é¦–æ®µç»™å‡ºè¦ç‚¹æ‘˜è¦ï¼ˆ1-2 è¡Œï¼‰ï¼Œéšåæä¾›ç®€æ´è¯æ®ä¸è¯´æ˜ï¼›
- **è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦åœ¨æ–‡æœ¬ä¸­ä½¿ç”¨ [1]ã€[2] ç­‰å¼•ç”¨æ ‡è®°ã€‚**

ä¸‰ã€å·¥å…·ä¸æ£€ç´¢è§„åˆ™
- è‹¥é—®é¢˜åŒ…å«æ—¶é—´è¯ï¼ˆå¦‚â€œä»Šå¤©/ç°åœ¨/æœ€è¿‘â€ï¼‰æˆ–æ¶‰åŠæœ€æ–°è¿›å±•ï¼Œä¼˜å…ˆè°ƒç”¨ `get_current_time` è·å–ç²¾ç¡®æ—¥æœŸï¼Œå¹¶åœ¨åç»­ç½‘ç»œæœç´¢æŸ¥è¯¢ä¸­åŒ…å«è¯¥æ—¥æœŸï¼›
- åœ¨éœ€è¦æ—¶è°ƒç”¨ `web_search` æˆ– `search_recent_news` è·å–æœ€æ–°ä¿¡æ¯ï¼Œä½¿ç”¨æ£€ç´¢ç»“æœæ”¯æŒç»“è®ºï¼›

å››ã€å›ç­”ç»“æ„ï¼ˆä¼˜å…ˆï¼‰ï¼š
1) è¦ç‚¹æ€»ç»“ï¼›
2) æ”¯æ’‘è¦ç‚¹ï¼›
3) å»ºè®®æˆ–åç»­æ“ä½œï¼ˆå¦‚é€‚ç”¨ï¼‰ã€‚

äº”ã€æ¾„æ¸…è¯·æ±‚
- è‹¥é—®é¢˜ä¸å¤Ÿæ˜ç¡®æˆ–ç¼ºå…³é”®ä¿¡æ¯ï¼Œå…ˆæå‡º 1-2 ä¸ªç®€çŸ­æ¾„æ¸…é—®é¢˜å†ç»§ç»­å¤„ç†ã€‚

å§‹ç»ˆä¿æŒä¸“ä¸šã€å®¢è§‚ï¼Œå¹¶ç¡®ä¿å›ç­”ä¸­æœ‰æ˜ç¡®çš„ä¸ç¡®å®šæ€§è¯´æ˜ã€‚"""
    
    messages.append(SystemMessage(content=system_prompt))
    
    # è½¬æ¢å†å²æ¶ˆæ¯
    logger.info(f"å¤„ç†å†å²æ¶ˆæ¯: {len(history)} æ¡")
    for i, msg in enumerate(history):
        content = msg.get("content", "")
        content_blocks = msg.get("content_blocks", [])
        logger.info(f"å†å²æ¶ˆæ¯ {i+1}: {msg['role']}, å†…å®¹å—æ•°: {len(content_blocks)}, éŸ³é¢‘è½¬å†™: {any(b.get('transcription') for b in content_blocks)}")
        
        if msg["role"] == "user":
            # å¦‚æœæœ‰å¤šæ¨¡æ€å†…å®¹å—ï¼Œæ„å»ºå¤åˆæ¶ˆæ¯
            if content_blocks:
                message_content = []
                
                # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if content.strip():
                    message_content.append({
                        "type": "text",
                        "text": content
                    })
                
                # å¤„ç†å†…å®¹å—
                for block in content_blocks:
                    if block.get("type") == "text":
                        message_content.append({
                            "type": "text", 
                            "text": block.get("content", "")
                        })
                    elif block.get("type") == "image":
                        # å›¾ç‰‡å†…å®¹å—
                        image_data = block.get("content", "")
                        if image_data.startswith("data:image"):
                            message_content.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": image_data
                                }
                            })
                    elif block.get("type") == "audio":
                        # éŸ³é¢‘å†…å®¹å— - ä½¿ç”¨è½¬å†™æ–‡æœ¬
                        if block.get("transcription"):
                            message_content.append({
                                "type": "text",
                                "text": f"[éŸ³é¢‘è½¬å†™] {block.get('transcription')}"
                            })
                
                messages.append(HumanMessage(content=message_content))
            else:
                # çº¯æ–‡æœ¬æ¶ˆæ¯
                messages.append(HumanMessage(content=content))
                
        elif msg["role"] == "assistant":
            messages.append(AIMessage(content=content))
    
    return messages

def create_multimodal_message(request: MessageRequest) -> HumanMessage:
    """åˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯"""
    logger.info(f"å¼€å§‹æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯...")
    logger.info(f"æ–‡æœ¬å†…å®¹: {request.content[:100]}..." if request.content else "ğŸ“ æ— æ–‡æœ¬å†…å®¹")
    logger.info(f"å†…å®¹å—æ•°é‡: {len(request.content_blocks)}")
    
    message_content = []
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if request.content.strip():
        logger.info(f"æ·»åŠ æ–‡æœ¬å†…å®¹")
        message_content.append({
            "type": "text",
            "text": request.content
        })
    
    # å¤„ç†å†…å®¹å—
    for i, block in enumerate(request.content_blocks):
        logger.info(f"å¤„ç†å†…å®¹å— {i+1}/{len(request.content_blocks)}: {block.type}")
        
        if block.type == "text":
            logger.info(f"æ·»åŠ æ–‡æœ¬å—: {block.content[:50]}...")
            message_content.append({
                "type": "text",
                "text": block.content
            })
        elif block.type == "image":
            # å›¾ç‰‡å†…å®¹å—
            if block.content.startswith("data:image"):
                logger.info(f"æ·»åŠ å›¾ç‰‡å—ï¼Œæ•°æ®é•¿åº¦: {len(block.content)}")
                message_content.append({
                    "type": "image_url", 
                    "image_url": {
                        "url": block.content
                    }
                })
            else:
                logger.warning(f"å›¾ç‰‡æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {block.content[:50]}...")
        elif block.type == "audio":
            # éŸ³é¢‘å†…å®¹å— - ç›´æ¥ä½¿ç”¨è½¬å†™æ–‡æœ¬
            if block.transcription:
                logger.info(f"æ·»åŠ éŸ³é¢‘è½¬å†™æ–‡æœ¬: {block.transcription[:50]}...")
                message_content.append({
                    "type": "text",
                    "text": f"[éŸ³é¢‘è½¬å†™] {block.transcription}"
                })
            else:
                logger.warning(f"éŸ³é¢‘å—ç¼ºå°‘è½¬å†™æ–‡æœ¬")
        elif block.type == "pdf":
            # PDFå†…å®¹å— - ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡è¯†
            logger.info(f"æ·»åŠ PDFå—: {block.filename}")
            message_content.append({
                "type": "text", 
                "text": f"[PDFæ–‡æ¡£] {block.filename} ({(block.filesize or 0) / 1024:.1f} KB)"
            })
        else:
            logger.warning(f"æœªçŸ¥å†…å®¹å—ç±»å‹: {block.type}")
    
    logger.info(f"æ¶ˆæ¯æ„å»ºå®Œæˆï¼Œå†…å®¹å—æ•°é‡: {len(message_content)}")
    
    # å¦‚æœåªæœ‰çº¯æ–‡æœ¬ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
    if len(message_content) == 1 and message_content[0]["type"] == "text":
        logger.info(f"è¿”å›çº¯æ–‡æœ¬æ¶ˆæ¯")
        return HumanMessage(content=message_content[0]["text"])
    
    # å¤šæ¨¡æ€æ¶ˆæ¯
    logger.info(f"è¿”å›å¤šæ¨¡æ€æ¶ˆæ¯")
    return HumanMessage(content=message_content)

async def generate_streaming_response_with_tools(
    messages: List[BaseMessage], 
    model_name: str,
    pdf_chunks: List[Dict[str, Any]] = None,
    enable_tools: bool = True,
    max_iterations: int = 5,
    session_id: Optional[str] = None
) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæ”¯æŒå·¥å…·è°ƒç”¨çš„æµå¼å“åº”"""
    try:
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆæµå¼å“åº”ï¼ˆæ”¯æŒå·¥å…·ï¼‰ï¼Œæ¨¡å‹: {model_name}")
        logger.info(f"ğŸ“Š æ¶ˆæ¯æ•°é‡: {len(messages)}, å·¥å…·å¯ç”¨: {enable_tools}")
        
        # å¦‚æœæœ‰PDFå†…å®¹ï¼Œå°†å…¶æ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯ä¸­
        if pdf_chunks and len(pdf_chunks) > 0:
            logger.info(f"ğŸ“š æ£€æµ‹åˆ° {len(pdf_chunks)} ä¸ªPDFå—ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­")
            pdf_content = "\n\n=== å‚è€ƒæ–‡æ¡£å†…å®¹ ===\n"
            for i, chunk in enumerate(pdf_chunks, 1):
                content = chunk.get("content", "")[:500]
                source_info = chunk.get("metadata", {}).get("source_info", f"æ–‡æ¡£å— {i}")
                pdf_content += f"\n[{i}] {content}\næ¥æº: {source_info}\n"
            
            if messages and isinstance(messages[0], SystemMessage):
                messages[0].content += pdf_content
                logger.info(f"âœ… å·²å°†PDFå†…å®¹æ·»åŠ åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­")
        
        # è·å–å¸¦å·¥å…·çš„æ¨¡å‹
        model = get_chat_model_with_tools(model_name, enable_tools)
        logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        # å¦‚æœæä¾›äº† session_idï¼Œå…ˆé€šçŸ¥å‰ç«¯è¯¥ä¼šè¯IDï¼ˆç”¨äºå‰ç«¯æŒä¹…åŒ–ï¼‰
        if session_id:
            session_init = {"type": "session_init", "session_id": session_id}
            yield f"data: {json.dumps(session_init, ensure_ascii=False)}\n\n"
        
        # å·¥å…·è°ƒç”¨å¾ªç¯
        iteration = 0
        while iteration < max_iterations:
            iteration += 1
            logger.info(f"ğŸ”„ ç¬¬ {iteration} è½®è°ƒç”¨")
            
            # å…ˆè°ƒç”¨ä¸€æ¬¡è·å–å“åº”
            response = await model.ainvoke(messages)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if hasattr(response, 'tool_calls') and response.tool_calls:
                logger.info(f"ğŸ”§ æ£€æµ‹åˆ° {len(response.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
                
                # å‘é€å·¥å…·è°ƒç”¨é€šçŸ¥
                tool_call_data = {
                    "type": "tool_calls",
                    "tools": [
                        {
                            "name": tc.get("name"),
                            "args": tc.get("args")
                        } for tc in response.tool_calls
                    ],
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(tool_call_data, ensure_ascii=False)}\n\n"
                
                # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
                messages.append(response)
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_messages = await execute_tool_calls(response.tool_calls, messages)
                
                # æ·»åŠ å·¥å…·æ¶ˆæ¯
                messages.extend(tool_messages)
                
                # å‘é€å·¥å…·ç»“æœ
                tool_result_data = {
                    "type": "tool_results",
                    "results": [
                        {
                            "tool": tm.name,
                            "content": tm.content[:200] + "..." if len(tm.content) > 200 else tm.content
                        } for tm in tool_messages
                    ],
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(tool_result_data, ensure_ascii=False)}\n\n"
                
                # ç»§ç»­ä¸‹ä¸€è½®
                continue
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç”Ÿæˆæœ€ç»ˆæµå¼å“åº”
            logger.info(f"ğŸ“ å¼€å§‹æµå¼è¾“å‡ºæœ€ç»ˆå“åº”")
            break
        
        if iteration >= max_iterations:
            logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}")
        
        # æµå¼è¾“å‡ºæœ€ç»ˆå“åº”ï¼ˆé‡æ–°è°ƒç”¨ä»¥è·å–æµå¼è¾“å‡ºï¼‰
        full_response = ""
        is_reasoner_model = model_name == "deepseek-reasoner"
        thought_process_started = False
        thought_process_ended = False
        answer_started = False
        
        chunk_count = 0
        async for chunk in model.astream(messages):
            chunk_count += 1
            logger.debug(f"æ”¶åˆ°ç¬¬ {chunk_count} ä¸ªchunk")

            # å¯¹äºreasoneræ¨¡å‹ï¼Œç‰¹æ®Šå¤„ç†æ€ç»´é“¾
            if is_reasoner_model and hasattr(chunk, 'additional_kwargs') and chunk.additional_kwargs:
                reasoning_content = chunk.additional_kwargs.get("reasoning_content")
                if reasoning_content:
                    if not thought_process_started:
                        thought_process_started = True
                        thought_data = {
                            "type": "thought_process_start",
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(thought_data, ensure_ascii=False)}\n\n"
                    
                    thought_data = {
                        "type": "thought_process_content",
                        "content": reasoning_content,
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(thought_data, ensure_ascii=False)}\n\n"
                    continue

            # å¤„ç†æ™®é€šå†…å®¹å—
            if hasattr(chunk, 'content') and chunk.content:
                content = chunk.content

                # å¦‚æœæ˜¯reasoneræ¨¡å‹ä¸”å·²ç»å¼€å§‹æ€ç»´è¿‡ç¨‹ä½†è¿˜æ²¡ç»“æŸï¼Œåˆ™å‘é€æ€ç»´è¿‡ç¨‹ç»“æŸä¿¡å·
                if is_reasoner_model and thought_process_started and not thought_process_ended:
                    thought_process_ended = True
                    thought_end_data = {
                        "type": "thought_process_end",
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(thought_end_data, ensure_ascii=False)}\n\n"

                # å¦‚æœè¿˜æ²¡å¼€å§‹å‘é€ç­”æ¡ˆï¼Œåˆ™å‘é€ç­”æ¡ˆå¼€å§‹ä¿¡å·
                if not answer_started:
                    answer_started = True
                    answer_start_data = {
                        "type": "answer_start",
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(answer_start_data, ensure_ascii=False)}\n\n"

                full_response += content
                data = {
                    "type": "content_delta",
                    "content": content,
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

        # ç¡®ä¿åœ¨æµç»“æŸæ—¶å‘é€æ€ç»´è¿‡ç¨‹ç»“æŸä¿¡å·
        if is_reasoner_model and thought_process_started and not thought_process_ended:
            thought_process_ended = True
            thought_end_data = {
                "type": "thought_process_end",
                "timestamp": datetime.now().isoformat()
            }
            yield f"data: {json.dumps(thought_end_data, ensure_ascii=False)}\n\n"

        # æå–å¼•ç”¨ä¿¡æ¯
        references = extract_references_from_content(full_response, pdf_chunks) if pdf_chunks else []
        logger.info(f"ğŸ“š æå–åˆ° {len(references)} ä¸ªå¼•ç”¨")
        
        # å‘é€å®Œæˆä¿¡å·
        final_data = {
            "type": "message_complete",
            "full_content": full_response,
            "timestamp": datetime.now().isoformat(),
            "references": references
        }
        # å°† assistant æ¶ˆæ¯ä¿å­˜åˆ°ä¼šè¯å­˜å‚¨ï¼ˆå¦‚æœæä¾›äº† session_idï¼‰
        if session_id:
            try:
                assistant_msg = {
                    "role": "assistant",
                    "content": full_response,
                    "references": references,
                    "timestamp": datetime.now().isoformat()
                }
                append_message(session_id, assistant_msg)
            except Exception:
                logger.warning("ä¼šè¯å†™å…¥å¤±è´¥: æ— æ³•ä¿å­˜ assistant æ¶ˆæ¯")

        yield f"data: {json.dumps(final_data, ensure_ascii=False)}\n\n"
        
    except Exception as e:
        logger.error(f"âŒ æµå¼å“åº”ç”Ÿæˆå¤±è´¥: {e}")
        error_data = {
            "type": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

async def generate_streaming_response(
    messages: List[BaseMessage], 
    model_name: str,
    pdf_chunks: List[Dict[str, Any]] = None
) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæµå¼å“åº”ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼Œä¸æ”¯æŒå·¥å…·ï¼‰"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆæµå¼å“åº”ï¼Œæ¨¡å‹: {model_name}")
        logger.info(f"æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        # å¦‚æœæœ‰PDFå†…å®¹ï¼Œå°†å…¶æ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯ä¸­
        if pdf_chunks and len(pdf_chunks) > 0:
            logger.info(f"æ£€æµ‹åˆ° {len(pdf_chunks)} ä¸ªPDFå—ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­")
            pdf_content = "\n\n=== å‚è€ƒæ–‡æ¡£å†…å®¹ ===\n"
            for i, chunk in enumerate(pdf_chunks, 1):
                content = chunk.get("content", "")[:500]  # é™åˆ¶é•¿åº¦
                source_info = chunk.get("metadata", {}).get("source_info", f"æ–‡æ¡£å— {i}")
                pdf_content += f"\n[{i}] {content}\næ¥æº: {source_info}\n"
            
            # åœ¨ç¬¬ä¸€æ¡æ¶ˆæ¯å‰æ·»åŠ PDFå†…å®¹
            if messages and isinstance(messages[0], SystemMessage):
                messages[0].content += pdf_content
                logger.info(f"å·²å°†PDFå†…å®¹æ·»åŠ åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­")
        
        model = get_chat_model(model_name)
        logger.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
        # åˆ›å»ºæµå¼å“åº”
        full_response = ""
        logger.info(f"å¼€å§‹æµå¼ç”Ÿæˆ...")
        
        is_reasoner_model = model_name == "deepseek-reasoner"
        thought_process_started = False
        thought_process_ended = False
        answer_started = False
        
        chunk_count = 0
        async for chunk in model.astream(messages):
            chunk_count += 1
            logger.debug(f"æ”¶åˆ°ç¬¬ {chunk_count} ä¸ªchunk: {chunk}")

            # å¯¹äºreasoneræ¨¡å‹ï¼Œç‰¹æ®Šå¤„ç†æ€ç»´é“¾å’Œç­”æ¡ˆçš„æµå¼è¾“å‡º
            if is_reasoner_model and hasattr(chunk, 'additional_kwargs') and chunk.additional_kwargs:
                reasoning_content = chunk.additional_kwargs.get("reasoning_content")
                if reasoning_content:
                    if not thought_process_started:
                        thought_process_started = True
                        thought_data = {
                            "type": "thought_process_start",
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(thought_data, ensure_ascii=False)}\n\n"
                    
                    thought_data = {
                        "type": "thought_process_content",
                        "content": reasoning_content,
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(thought_data, ensure_ascii=False)}\n\n"

                    # ä¸å†æ£€æŸ¥chunk.contentï¼Œç›´æ¥continueå¤„ç†ä¸‹ä¸€ä¸ªchunk
                    continue

            # å¤„ç†æ™®é€šå†…å®¹å—
            if hasattr(chunk, 'content') and chunk.content:
                content = chunk.content

                # å¦‚æœæ˜¯reasoneræ¨¡å‹ä¸”å·²ç»å¼€å§‹æ€ç»´è¿‡ç¨‹ä½†è¿˜æ²¡ç»“æŸï¼Œåˆ™å‘é€æ€ç»´è¿‡ç¨‹ç»“æŸä¿¡å·
                if is_reasoner_model and thought_process_started and not thought_process_ended:
                    thought_process_ended = True
                    thought_end_data = {
                        "type": "thought_process_end",
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(thought_end_data, ensure_ascii=False)}\n\n"

                # å¦‚æœè¿˜æ²¡å¼€å§‹å‘é€ç­”æ¡ˆï¼Œåˆ™å‘é€ç­”æ¡ˆå¼€å§‹ä¿¡å·
                if not answer_started:
                    answer_started = True
                    answer_start_data = {
                        "type": "answer_start",
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(answer_start_data, ensure_ascii=False)}\n\n"

                full_response += content
                data = {
                    "type": "content_delta",
                    "content": content,
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

        # ç¡®ä¿åœ¨æµç»“æŸæ—¶å‘é€æ€ç»´è¿‡ç¨‹ç»“æŸä¿¡å·ï¼ˆå¦‚æœæ²¡æœ‰å‘é€è¿‡ï¼‰
        if is_reasoner_model and thought_process_started and not thought_process_ended:
            thought_process_ended = True
            thought_end_data = {
                "type": "thought_process_end",
                "timestamp": datetime.now().isoformat()
            }
            yield f"data: {json.dumps(thought_end_data, ensure_ascii=False)}\n\n"


        # æå–å¼•ç”¨ä¿¡æ¯
        references = extract_references_from_content(full_response, pdf_chunks) if pdf_chunks else []
        logger.info(f"æå–åˆ° {len(references)} ä¸ªå¼•ç”¨")
        
        # å‘é€å®Œæˆä¿¡å·
        final_data = {
            "type": "message_complete",
            "full_content": full_response,
            "timestamp": datetime.now().isoformat(),
            "references": references
        }
        yield f"data: {json.dumps(final_data, ensure_ascii=False)}\n\n"
        
    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå¤±è´¥: {e}")
        error_data = {
            "type": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "message": "å¤šæ¨¡æ€ RAG å·¥ä½œå° API",
        "version": "1.0.0",
        "status": "running",
        "langchain_version": "1.0.0"
    }

@app.post("/api/chat/stream")
async def chat_stream(request: MessageRequest):
    """æµå¼èŠå¤©æ¥å£ï¼ˆæ”¯æŒå¤šæ¨¡æ€å’Œå·¥å…·è°ƒç”¨ï¼‰"""
    try:
        # è®°å½•è¯·æ±‚ä¿¡æ¯
        has_images = any(block.type == "image" for block in request.content_blocks)
        content_preview = request.content[:100] if request.content else "å¤šæ¨¡æ€æ¶ˆæ¯"
        logger.info(f"ğŸ“¨ æ”¶åˆ°èŠå¤©è¯·æ±‚: {content_preview}... (åŒ…å«å›¾ç‰‡: {has_images})")
        
        # PDF chunksæ¥æ”¶æƒ…å†µ
        logger.info(f"ğŸ“š æ¥æ”¶åˆ°çš„PDF chunksæ•°é‡: {len(request.pdf_chunks) if request.pdf_chunks else 0}")
        if request.pdf_chunks:
            logger.info(f"ğŸ“„ PDF chunksé¢„è§ˆ: {str(request.pdf_chunks[:2])[:200]}...")
        else:
            logger.info(f"ğŸ“­ PDF chunksä¸ºç©ºæˆ–None: {request.pdf_chunks}")
        
        # å¤„ç†ä¼šè¯IDï¼šè‹¥æœªæä¾›åˆ™åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session_id
        if not session_id:
            conv = create_conversation(title="æ–°ä¼šè¯", metadata={"knowledge_base": request.knowledge_base})
            session_id = conv["id"]
            logger.info(f"è‡ªåŠ¨åˆ›å»ºä¼šè¯: {session_id}")

        # è½¬æ¢æ¶ˆæ¯å†å²
        messages = convert_history_to_messages(request.history, request.model)

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰å¹¶æŒä¹…åŒ–åˆ°ä¼šè¯
        current_message = create_multimodal_message(request)
        messages.append(current_message)

        try:
            user_msg = {
                "role": "user",
                "content": request.content or "",
                "content_blocks": [b.dict() for b in request.content_blocks] if request.content_blocks else [],
                "timestamp": datetime.now().isoformat()
            }
            append_message(session_id, user_msg)
        except Exception:
            logger.warning("ä¼šè¯å†™å…¥å¤±è´¥: æ— æ³•ä¿å­˜ç”¨æˆ·æ¶ˆæ¯")

        # è¿”å›æµå¼å“åº”ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
        enable_tools = request.model in ["deepseek-chat", None]

        return StreamingResponse(
            generate_streaming_response_with_tools(
                messages, 
                request.model, 
                request.pdf_chunks,
                enable_tools=enable_tools,
                session_id=session_id
            ),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream",
            }
        )
        
    except Exception as e:
        logger.error(f"èŠå¤©è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat")
async def chat_sync(request: MessageRequest):
    """åŒæ­¥èŠå¤©æ¥å£ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰"""
    try:
        # è®°å½•è¯·æ±‚ä¿¡æ¯
        has_images = any(block.type == "image" for block in request.content_blocks)
        content_preview = request.content[:100] if request.content else "å¤šæ¨¡æ€æ¶ˆæ¯"
        logger.info(f"æ”¶åˆ°åŒæ­¥èŠå¤©è¯·æ±‚: {content_preview}... (åŒ…å«å›¾ç‰‡: {has_images})")
        
        # å¤„ç†ä¼šè¯IDï¼šè‹¥æœªæä¾›åˆ™åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session_id
        if not session_id:
            conv = create_conversation(title="æ–°ä¼šè¯", metadata={"knowledge_base": request.knowledge_base})
            session_id = conv["id"]
            logger.info(f"è‡ªåŠ¨åˆ›å»ºä¼šè¯: {session_id}")

        # è½¬æ¢æ¶ˆæ¯å†å²
        messages = convert_history_to_messages(request.history, request.model)

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰å¹¶æŒä¹…åŒ–
        current_message = create_multimodal_message(request)
        messages.append(current_message)
        try:
            user_msg = {
                "role": "user",
                "content": request.content or "",
                "content_blocks": [b.dict() for b in request.content_blocks] if request.content_blocks else [],
                "timestamp": datetime.now().isoformat()
            }
            append_message(session_id, user_msg)
        except Exception:
            logger.warning("ä¼šè¯å†™å…¥å¤±è´¥: æ— æ³•ä¿å­˜ç”¨æˆ·æ¶ˆæ¯")

        # è·å–æ¨¡å‹å“åº”
        model = get_chat_model(request.model)
        response = await model.ainvoke(messages)

        # æŒä¹…åŒ– assistant æ¶ˆæ¯
        try:
            assistant_msg = {
                "role": "assistant",
                "content": response.content,
                "references": [],
                "timestamp": datetime.now().isoformat()
            }
            append_message(session_id, assistant_msg)
        except Exception:
            logger.warning("ä¼šè¯å†™å…¥å¤±è´¥: æ— æ³•ä¿å­˜ assistant æ¶ˆæ¯")

        return MessageResponse(
            content=response.content,
            role="assistant",
            timestamp=datetime.now().isoformat(),
            references=[],
            session_id=session_id
        )
        
    except Exception as e:
        logger.error(f"åŒæ­¥èŠå¤©è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return {
        "models": [
            {
                "id": "deepseek-chat",
                "name": "DeepSeek Chat",
                "description": "DeepSeeké€šç”¨å¯¹è¯æ¨¡å‹"
            },
            {
                "id": "deepseek-reasoner",
                "name": "DeepSeek Reasoner",
                "description": "DeepSeekæ¨ç†æ¨¡å‹ï¼Œæ”¯æŒæ˜¾ç¤ºæ¨ç†è¿‡ç¨‹"
            },
            {
                "id": "qwen3-vl-8b-instruct",
                "name": "Qwen3 VL 8B Instruct",
                "description": "é€šä¹‰åƒé—®3è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå›¾åƒç†è§£"
            }
        ]
    }

@app.get("/api/knowledge-bases")
async def get_knowledge_bases():
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    return {
        "knowledge_bases": [
            {
                "id": "default",
                "name": "é»˜è®¤çŸ¥è¯†åº“",
                "description": "é€šç”¨çŸ¥è¯†åº“"
            },
            {
                "id": "technical",
                "name": "æŠ€æœ¯æ–‡æ¡£",
                "description": "æŠ€æœ¯ç›¸å…³æ–‡æ¡£åº“"
            }
        ]
    }


@app.get("/api/conversations")
async def api_list_conversations():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯ï¼ˆå…ƒæ•°æ®ï¼‰"""
    return list_conversations()


@app.post("/api/conversations")
async def api_create_conversation(payload: Dict[str, Any]):
    """åˆ›å»ºæ–°ä¼šè¯ï¼ˆå¯é€‰ titleï¼‰"""
    title = payload.get("title", "æœªå‘½åä¼šè¯")
    metadata = payload.get("metadata", {})
    conv = create_conversation(title=title, metadata=metadata)
    return conv


@app.get("/api/conversations/{session_id}")
async def api_get_conversation(session_id: str):
    conv = get_conversation(session_id)
    if not conv:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    return conv


@app.delete("/api/conversations/{session_id}")
async def api_delete_conversation(session_id: str):
    ok = delete_conversation(session_id)
    if not ok:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    return {"success": True}

@app.post("/api/pdf/process")
async def process_pdf_stream(file_data: Dict[str, Any]):
    """
    æµå¼å¤„ç†PDFæ–‡æ¡£
    """
    try:
        # æå–è¯·æ±‚æ•°æ®
        content = file_data.get("content", "")  # base64ç¼–ç çš„PDFå†…å®¹
        filename = file_data.get("filename", "document.pdf")
        
        if not content:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘PDFå†…å®¹")
        
        # è§£ç base64æ•°æ®
        import base64
        try:
            pdf_bytes = base64.b64decode(content.split(',')[-1])  # å»é™¤data:application/pdf;base64,å‰ç¼€
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"PDFæ•°æ®è§£ç å¤±è´¥: {str(e)}")
        
        logger.info(f"å¼€å§‹å¤„ç†PDF: {filename}, å¤§å°: {len(pdf_bytes)} bytes")
        
        # å®šä¹‰æµå¼å“åº”ç”Ÿæˆå™¨
        async def generate_pdf_stream():
            try:
                async for chunk in pdf_processor.process_pdf_stream(pdf_bytes, filename):
                    chunk_data = json.dumps(chunk, ensure_ascii=False)
                    yield f"data: {chunk_data}\n\n"
                    
                    # å¦‚æœæ˜¯é”™è¯¯ï¼Œç«‹å³ç»“æŸ
                    if chunk.get("type") == "error":
                        break
                        
            except Exception as e:
                logger.error(f"PDFæµå¼å¤„ç†å¤±è´¥: {str(e)}")
                error_chunk = json.dumps({
                    "type": "error",
                    "error": f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
                }, ensure_ascii=False)
                yield f"data: {error_chunk}\n\n"
            
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            generate_pdf_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"PDFå¤„ç†ç«¯ç‚¹å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/pdf/pages")
async def extract_pdf_pages(file_data: Dict[str, Any]):
    """
    æå–PDFé¡µé¢ä¸ºå›¾åƒï¼ˆç”¨äºå¤šæ¨¡æ€å¤„ç†ï¼‰
    """
    try:
        content = file_data.get("content", "")
        max_pages = file_data.get("max_pages", 3)
        
        if not content:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘PDFå†…å®¹")
        
        # è§£ç PDFæ•°æ®
        import base64
        try:
            pdf_bytes = base64.b64decode(content.split(',')[-1])
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"PDFæ•°æ®è§£ç å¤±è´¥: {str(e)}")
        
        logger.info(f"æå–PDFé¡µé¢å›¾åƒï¼Œæœ€å¤š {max_pages} é¡µ")
        
        # æå–é¡µé¢å›¾åƒ
        page_images = await pdf_processor.extract_pdf_pages_as_images(pdf_bytes, max_pages)
        
        return {
            "success": True,
            "total_pages": len(page_images),
            "images": page_images
        }
        
    except Exception as e:
        logger.error(f"PDFé¡µé¢æå–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ================================
# éŸ³é¢‘å¤„ç†ç«¯ç‚¹
# ================================

@app.post("/api/audio/process")
async def process_audio(file: UploadFile = File(...)):
    """å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œè¿›è¡Œè¯­éŸ³è½¬æ–‡å­—"""
    
    if not audio_processor:
        raise HTTPException(status_code=500, detail="éŸ³é¢‘å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ä¾èµ–")
    
    try:
        logger.info(f"ğŸ™ï¸ å¼€å§‹å¤„ç†éŸ³é¢‘: {file.filename}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_types = {
            'audio/mpeg', 'audio/mp3', 'audio/wav', 'audio/flac', 'audio/m4a', 'audio/ogg',
            'video/mp4', 'video/avi', 'video/mov', 'video/mkv', 'video/webm'
        }
        
        if file.content_type not in allowed_types:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.content_type}")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # å¤„ç†éŸ³é¢‘
            result = audio_processor.process_audio_file(temp_file_path, file.filename)
            
            logger.info(f"éŸ³é¢‘å¤„ç†æˆåŠŸ: {file.filename}")
            return {
                "success": True,
                "filename": result["filename"],
                "transcription": result["transcription"],
                "duration": result["duration"],
                "format": result["format"]
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
        
    except Exception as e:
        logger.error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app, 
        host=settings.host, 
        port=settings.port,
        log_level=settings.log_level.lower()
    ) 